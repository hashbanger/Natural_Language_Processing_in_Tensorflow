{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_in_Texts.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lHP48ZRLeK9",
        "colab_type": "text"
      },
      "source": [
        "### Vanilla tokenizing using Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgZJ3snwI5DT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u89vOskuKnmZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a31b1ba-2c82-404c-818d-527cb17c5bae"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PvwbTlOK16p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [\n",
        "             'I love my dog',\n",
        "             'I love my cat'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT2cPXjGK_-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnbttjLjLoVn",
        "colab_type": "text"
      },
      "source": [
        "**Note:** tokenizer does not stops tokenizing at 100 words, it will keep all but won't use above 100 in later stages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Koz3TvQ5LFaX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92bf9210-db34-44a0-8ce8-83862ca22830"
      },
      "source": [
        "print(word_index)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFZndc1-L_iL",
        "colab_type": "text"
      },
      "source": [
        "____"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdfxrHmLLY0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [\n",
        "             'I love my dog',\n",
        "             'I love my cat',\n",
        "             'You love my dog!'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvUiIRGtL7Vh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5OkgXkuMKDl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8720300-93c6-4599-b626-f8874fe76efc"
      },
      "source": [
        "print(word_index)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9skGo2SzMPDO",
        "colab_type": "text"
      },
      "source": [
        "______"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm-iiY6BMLdX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [\n",
        "             'I love my dog',\n",
        "             'I love my cat',\n",
        "             'You love my dog!',\n",
        "             'Do you think my dog is amazing?'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BLFR1VlM7UH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words= 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5UHkjviNIcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO5xjeQRNOmi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "82f4e9d7-29fa-4e4b-e7a2-5fbb618eb8cc"
      },
      "source": [
        "print(word_index, end = '\\n\\n')\n",
        "print(sequences)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
            "\n",
            "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkU5tHpXNfZx",
        "colab_type": "text"
      },
      "source": [
        "We have the sentences with each word replaced with its corresponnding numeric token. \n",
        "\n",
        "Now when we test the data for sentences having some words out of the vocabulary, we can observe some interesting results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3_-SGlkNXh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = ['i really love my dog',\n",
        "            'my dog loves my manatee']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTzxUSxVOmUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "079cfef4-5c87-4189-e7e2-02f71c3c1efd"
      },
      "source": [
        "print(word_index, end = '\\n\\n')\n",
        "print(tokenizer.texts_to_sequences(test_data))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
            "\n",
            "[[4, 2, 1, 3], [1, 3, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pX4PQ1HvO5EE",
        "colab_type": "text"
      },
      "source": [
        "We can observe that out of vocabulary words are skipped, which is not really the best way to handle those.\n",
        "\n",
        "We will replace such words with a manual token <OOV> which just indicates the out of vocabulary words.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "400lkL3uO33t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=100, oov_token= '<OOV>')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THibq7o2Pf80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQs3QdQ3PmMA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "379c3165-622f-496f-a3a2-a5dcba51bacf"
      },
      "source": [
        "print(word_index, end = '\\n\\n')\n",
        "print(tokenizer.texts_to_sequences(test_data))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "\n",
            "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHwz3zyZP3eE",
        "colab_type": "text"
      },
      "source": [
        "Now we can observe that \\<OOV> has been added in word index and has been replaced in the sequencing as well.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb03VWQyQiFj",
        "colab_type": "text"
      },
      "source": [
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqhpMOUNQjTs",
        "colab_type": "text"
      },
      "source": [
        "Now we have uneven length sentences, so we will have to convert them in uniform length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCePuJVcPulG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5Dadwq8QXDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=100, oov_token= '<OOV>')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XM7pGlxNQ83h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkQx5noMRDs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded = pad_sequences(sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tuc-jfyRKJJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "efdd0ec6-f848-4eeb-8e6e-ded4ca53943c"
      },
      "source": [
        "print(word_index, end = '\\n\\n')\n",
        "print(sequences, end = '\\n\\n')\n",
        "print(padded)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "\n",
            "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "\n",
            "[[ 0  0  0  5  3  2  4]\n",
            " [ 0  0  0  5  3  2  7]\n",
            " [ 0  0  0  6  3  2  4]\n",
            " [ 8  6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu-9nV-rRWo9",
        "colab_type": "text"
      },
      "source": [
        "We can observe that the sentences has been transformed into the length equal to that of the longest sequence, with the shorter sentences being padded from the beginning.  \n",
        "\n",
        "We can pad from the end as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLsKJH1WRU4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded = pad_sequences(sequences, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc7ysjlFRxP9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "dd1e1ece-2a7d-4e96-95b6-0187fec4b6cb"
      },
      "source": [
        "print(word_index, end = '\\n\\n')\n",
        "print(sequences, end = '\\n\\n')\n",
        "print(padded)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "\n",
            "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "\n",
            "[[ 5  3  2  4  0  0  0]\n",
            " [ 5  3  2  7  0  0  0]\n",
            " [ 6  3  2  4  0  0  0]\n",
            " [ 8  6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-55tYvuKR4xQ",
        "colab_type": "text"
      },
      "source": [
        "We can also truncate the lengths as per our need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ssvXONj4R6Ue",
        "colab": {}
      },
      "source": [
        "padded = pad_sequences(sequences, padding='post', maxlen= 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "29f6720f-f2cd-4347-a76d-b659ec0f9c99",
        "id": "G5btb6d9SDBG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "print(word_index, end = '\\n\\n')\n",
        "print(sequences, end = '\\n\\n')\n",
        "print(padded)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "\n",
            "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "\n",
            "[[ 5  3  2  4  0]\n",
            " [ 5  3  2  7  0]\n",
            " [ 6  3  2  4  0]\n",
            " [ 9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXLz9BrSGez",
        "colab_type": "text"
      },
      "source": [
        "The sentences have been truncated from the beginning. Consider the truncating before padding not after padding.\n",
        "\n",
        "We can also truncate from the end."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81enismFR-Sg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded = pad_sequences(sequences, padding='post', maxlen= 5, truncating= 'post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lZMXoSjSlMp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "f0bdf40e-b431-4bab-f87a-36f3ca49f1f5"
      },
      "source": [
        "print(word_index, end = '\\n\\n')\n",
        "print(sequences, end = '\\n\\n')\n",
        "print(padded)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "\n",
            "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
            "\n",
            "[[5 3 2 4 0]\n",
            " [5 3 2 7 0]\n",
            " [6 3 2 4 0]\n",
            " [8 6 9 2 4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNhSOlO1T6jP",
        "colab_type": "text"
      },
      "source": [
        "***de nada!***\n",
        "\n"
      ]
    }
  ]
}